{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Creating a RAG system with GeminiAPI\n",
        "\n",
        "* Financial Q&A dataset from kaggle - https://www.kaggle.com/datasets/yousefsaeedian/financial-q-and-a-10k\n",
        "\n",
        "* API key for Gemini can be obtained from Google AI Studio."
      ],
      "metadata": {
        "id": "2hzVGrPHXQi-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lsHLQu3lXN8-"
      },
      "outputs": [],
      "source": [
        "!pip install -qU \"google-genai==1.7.0\" \"chromadb==0.6.3\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing the dataset\n",
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"yousefsaeedian/financial-q-and-a-10k\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "id": "eMAXC0oudNA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /root/.cache/kagglehub/datasets/yousefsaeedian/financial-q-and-a-10k/versions/1"
      ],
      "metadata": {
        "id": "l5RfyJtNdQvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(path+\"/Financial-QA-10k.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "gD0fcPlgdlzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Keeping only the answers, tickers and filing information in the dataset\n",
        "df['train'] = df['answer'] + ' Ticker: ' + df['ticker'] + \". Filing: \" + df['filing']\n",
        "df['train'][0]"
      ],
      "metadata": {
        "id": "NG3JmOs9d-DG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Removing null and duplicate values in the dataset, since they can cause errors during embedding generation\n",
        "df.drop_duplicates(inplace = True)\n",
        "df.dropna(inplace = True)"
      ],
      "metadata": {
        "id": "azQe96bZ0XIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing libraries\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "from IPython.display import Markdown"
      ],
      "metadata": {
        "id": "XcRrpWUtXaDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing the secret or api key. Note - Make sure that the API key has been added to Google Collab Secrets\n",
        "from google.colab import userdata\n",
        "GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')"
      ],
      "metadata": {
        "id": "mD5W028IXyai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Listing all the models available that can work with embeddings\n",
        "client = genai.Client(api_key = GEMINI_API_KEY)\n",
        "\n",
        "for m in client.models.list():\n",
        "  if \"embedContent\" in m.supported_actions:\n",
        "    print(m.name)"
      ],
      "metadata": {
        "id": "_X5tLkOYX9d8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Using the text-embedding-004 model to create embeddings\n",
        "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
        "from google.api_core import retry\n",
        "from google.genai import types\n",
        "\n",
        "#Defining a helper to retry when the per-minute quota is reached\n",
        "is_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in (429,503))\n",
        "\n",
        "#Creating a class\n",
        "class GeminiEmbeddings(EmbeddingFunction):\n",
        "  #Specifying whether we are working with documents or queries\n",
        "  document_mode = True\n",
        "\n",
        "  #Creating a decorator that adds the retry behaviour to a function\n",
        "  @retry.Retry(predicate = is_retriable)\n",
        "  def __call__(self, input: Documents) -> Embeddings: #Creating a function that takes documents and returns embeddings\n",
        "    #Checking the document mode\n",
        "    if self.document_mode:\n",
        "      embedding_task = \"retrieval_document\"\n",
        "    else:\n",
        "      embedding_task = \"retrieval_query\"\n",
        "\n",
        "    #Configuring the response as per the model, input documents and document mode\n",
        "    response = client.models.embed_content(\n",
        "        model = \"models/text-embedding-004\",\n",
        "        contents = input,\n",
        "        config = types.EmbedContentConfig(\n",
        "            task_type = embedding_task,\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    #Returning the embeddings of the documents\n",
        "    return [e.values for e in response.embeddings]"
      ],
      "metadata": {
        "id": "unwvuY2rf2Ga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a database client for chromadb and populate it with the embeddings from class created above\n",
        "import chromadb\n",
        "\n",
        "db_name = 'financial_db'\n",
        "\n",
        "embed_fn = GeminiEmbeddings() #Embedding function to be used is the class that was defined above\n",
        "embed_fn.document_mode = True\n",
        "\n",
        "#Creating a client and a collection with specified name and embedding function\n",
        "chroma_client = chromadb.Client()\n",
        "db = chroma_client.get_or_create_collection(name = db_name, embedding_function = embed_fn)\n",
        "\n",
        "#Adding the documents in the database\n",
        "#Only 1st 100 documents are added, due to API restrictions on creating more embeddings\n",
        "db.add(documents = df['train'][:100].to_list(), ids = [str(i) for i in range(100)])"
      ],
      "metadata": {
        "id": "OaelYvUlpcWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Verifying that the documents were added\n",
        "db.count()"
      ],
      "metadata": {
        "id": "EqPTa6Gur-AF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Viewing the 1st document added\n",
        "db.peek(1)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "j9GEkRsjsKO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Switch to query mode when querying our collection\n",
        "embed_fn.document_mode = False\n",
        "\n",
        "#Search the chroma db for the specified query\n",
        "query = \"Explain NVIDIA's CUDA programming model and when did it create the GPU?\"\n",
        "results = db.query(query_texts=[query], n_results = 5) #Retrieving 5 most relevant answers\n",
        "\n",
        "#Obtain only the document from the results and not the other data\n",
        "[all_passages] = results['documents']\n",
        "\n",
        "#Print the 1st result\n",
        "Markdown(all_passages[0])"
      ],
      "metadata": {
        "id": "ATA33n5hsmp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Since we can obtain the relevant passage from the database for the query, we now pass it to Gemini to generate the final result\n",
        "\n",
        "#Converting a multiline query into a single line\n",
        "query_oneline = query.replace(\"\\n\",\" \")\n",
        "\n",
        "#Crafting a prompt\n",
        "prompt = f\"\"\"You are a helpful and informative bot that answers questions using text from the reference passage included below.\n",
        "Be sure to respond in a complete sentence, being comprehensive, including all relevant background information.\n",
        "Be sure to break down complicated concepts and strike a friendly and converstional tone. If the passage is irrelevant to the answer, you may ignore it.\n",
        "\n",
        "QUESTION: {query_oneline}\"\"\"\n",
        "\n",
        "#Add the retrieved passages to the prompt\n",
        "for passages in all_passages:\n",
        "  passage_oneline = passages.replace(\"\\n\",\" \")\n",
        "  prompt += f\"\\nPassage: {passage_oneline}\"\n",
        "\n",
        "print(prompt)"
      ],
      "metadata": {
        "id": "SpgDExMOuPLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Obtaining a response from the gemini-2.0-flash model from Google.\n",
        "answer = client.models.generate_content(\n",
        "    model = \"gemini-2.0-flash\",\n",
        "    contents = prompt\n",
        ")\n",
        "\n",
        "#Presenting the answer via Markdown\n",
        "Markdown(answer.text)"
      ],
      "metadata": {
        "id": "jgRmU7Ykw7vO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embeddings - 2nd Method\n",
        "* Since there are some restrictions on the number of API calls for the embedding function, we can use the sentence transformers in python to create our own embeddings.\n",
        "* These embeddings can then be saved via pickle, which can be used later, even when the session is completed, to avoid recalculations."
      ],
      "metadata": {
        "id": "BYglYeY9SvDg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "#Creating the embeddings\n",
        "embeddings = model.encode(list(df['train']), batch_size=64, show_progress_bar=True)"
      ],
      "metadata": {
        "id": "_hKCIOPVZzGs",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Saving the embeddings using pickle in the directory\n",
        "import pickle\n",
        "with open(\"embeddings.pkl\",\"wb\") as file:\n",
        "  pickle.dump(embeddings, file)"
      ],
      "metadata": {
        "id": "v9PSmYVtckWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a collection and adding the newly created embeddings\n",
        "db2 = chroma_client.get_or_create_collection(name = \"financial_db2\")\n",
        "db2.add(documents = list(df['train']), ids = [str(i) for i in range(len(df))], embeddings = embeddings)"
      ],
      "metadata": {
        "id": "AxhFVeqrcsXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db2.count()"
      ],
      "metadata": {
        "id": "TiHkiq2feIgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(df)"
      ],
      "metadata": {
        "id": "cRRPXlhkeKOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Querying the collection\n",
        "query = \"When did NVIDIA invent the GPU?\"\n",
        "results = db2.query(query_texts = query, n_results = 5)\n",
        "all_passages = results['documents']\n",
        "Markdown(results[\"documents\"][0][0])"
      ],
      "metadata": {
        "id": "iecrKJ6dUW09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Generating final result using Gemini\n",
        "query_oneline = query.replace(\"\\n\",\" \")\n",
        "\n",
        "#Crafting the prompt\n",
        "prompt = f\"\"\"You are a helpful and informative bot that answers questions using text from the reference passage included below.\n",
        "Be sure to respond in a complete sentence, being comprehensive, including all relevant background information.\n",
        "Be sure to break down complicated concepts and strike a friendly and converstional tone. If the passage is irrelevant to the answer, you may ignore it.\n",
        "\n",
        "QUESTION: {query_oneline}\"\"\"\n",
        "\n",
        "#Add the retrieved passages to the prompt\n",
        "for passages in all_passages[0]:\n",
        "  passage_oneline = passages.replace(\"\\n\",\" \")\n",
        "  prompt += f\"\\nPassage: {passage_oneline}\"\n",
        "\n",
        "print(prompt)"
      ],
      "metadata": {
        "id": "OEj9lckgqlJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a function to take user queries and returning the results using Gemini\n",
        "def query_gemini(query:str, n_results:int):\n",
        "  #Obtaining the search results for user query based on number of results decided by the user\n",
        "  results = db2.query(query_texts = query, n_results = n_results)\n",
        "  all_passages = results['documents']\n",
        "\n",
        "\n",
        "  #Crafting the prompt\n",
        "  query_oneline = query.replace(\"\\n\",\" \")\n",
        "\n",
        "  prompt = f\"\"\"You are a helpful and informative bot that answers questions using text from the reference passage included below.\n",
        "  Be sure to respond in a complete sentence, being comprehensive, including all relevant background information.\n",
        "  Be sure to break down complicated concepts and strike a friendly and converstional tone. If the passage is irrelevant to the answer, you may ignore it.\n",
        "\n",
        "  QUESTION: {query_oneline}\"\"\"\n",
        "\n",
        "  #Add the retrieved passages to the prompt\n",
        "  for passages in all_passages[0]:\n",
        "    passage_oneline = passages.replace(\"\\n\",\" \")\n",
        "    prompt += f\"\\nPassage: {passage_oneline}\"\n",
        "\n",
        "  #Generating answer\n",
        "  answer = client.models.generate_content(\n",
        "      model = \"gemini-2.0-flash\",\n",
        "      contents = prompt\n",
        "  )\n",
        "\n",
        "  print(answer.text)"
      ],
      "metadata": {
        "id": "pLvdiiQgq5vc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_gemini(\"Provide the financial results for Nvidia in 2023\", n_results = 5)"
      ],
      "metadata": {
        "id": "aEbaSsZzr1G3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}